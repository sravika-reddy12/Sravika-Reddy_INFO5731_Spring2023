{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "authorship_tag": "ABX9TyOOTptL3nSKY6GEJn0WaiAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sravika-reddy12/Sravika-Reddy_INFO5731_Spring2023/blob/main/Assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **INFO 5731 ASSIGNMENT 2**"
      ],
      "metadata": {
        "id": "lgsPpcCdqtaO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1 Write a python program to collect text data from either of the following sources and save the data into a csv file:(40 points)\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon.\n",
        "\n",
        "(2) Collect the top 10000 User Reviews of a film recently in 2023 or 2022 (you can choose any film) from IMDB.\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra\n",
        "\n",
        "(4) Collect the abstracts of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n",
        "\n",
        "(6) Collect the top 10000 tweets by using a hashtag (you can use any hashtag) from Twitter.\n",
        "\n",
        "[ ]\n"
      ],
      "metadata": {
        "id": "fUi5SRbZq2Kp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# specify the URL of the IMDB page for the Avatar movie reviews\n",
        "url = 'https://www.imdb.com/title/tt1630029/reviews/?ref_=tt_ql_urv'\n",
        "\n",
        "# send a GET request to the URL\n",
        "response = requests.get(url) # parse the HTML content using Beautiful Soup\n",
        "\n",
        "\n",
        "soup = BeautifulSoup(response.content, 'html.parser') # find the total number of user reviews\n",
        "\n",
        "\n",
        "total_reviews_str = soup.find('div', {'class': 'header'}).get_text()\n",
        "total_reviews_str = total_reviews_str.split()[0] # extract the first word of the string\n",
        "\n",
        "total_reviews = int(total_reviews_str.replace(',', ''))\n",
        "\n",
        "\n",
        "csv_file = open('Avatar_reviews.csv', 'w', newline='', encoding='utf-8') # create a CSV file to save the data\n",
        "csv_writer = csv.writer(csv_file)\n",
        "csv_writer.writerow(['Review'])\n",
        "\n",
        "# loop through all the pages of reviews and save them in the CSV file\n",
        "for page in range(1, min(1001, total_reviews // 10 + 2)):# generate the URL for the current page of reviews\n",
        "    \n",
        "    page_url = f'{url}&start={10*(page-1)}'\n",
        "    \n",
        "    page_response = requests.get(page_url)# send a GET request to the page URL\n",
        "   \n",
        "    page_soup = BeautifulSoup(page_response.content, 'html.parser') # parse the HTML content using Beautiful Soup\n",
        "    \n",
        "    reviews = page_soup.find_all('div', {'class': 'text show-more__control'}) # find all the user reviews on the page\n",
        "    \n",
        "    for review in reviews: # loop through all the reviews and save them in the CSV file\n",
        "        review_text = review.get_text().strip()\n",
        "        csv_writer.writerow([review_text])\n",
        "\n",
        "# close the CSV file\n",
        "csv_file.close()\n",
        "\n"
      ],
      "metadata": {
        "id": "U8ZMSopFj2hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "                   # read the CSV file into a DataFrame\n",
        "df = pd.read_csv('Avatar_reviews.csv')\n",
        "\n",
        "                   # print the first 100 rows of the DataFrame\n",
        "print(df.head(3000))"
      ],
      "metadata": {
        "id": "M5k-jLwzmC7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a python program to clean the text data you collected above and save the data in a new column in the csv file. The data cleaning steps include:(30 points).\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ],
      "metadata": {
        "id": "tudN6iMLrsEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "import nltk\n",
        "nltk.download()\n"
      ],
      "metadata": {
        "id": "Tftv9GODndsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuations \n",
        "#and special characters\n",
        "\n",
        "import re # importing\n",
        "#PUNCTUATIONS\n",
        "df['Reviews after Noise Removal'] = df['Review'].str.replace('[^\\w\\s]','')\n",
        "df['Reviews after Noise Removal'] = df['Reviews after Noise Removal'].apply(lambda x: ''.join(re.sub(r\"[^a-zA-Z0-9]+\", ' ', charctr) for charctr in x ))"
      ],
      "metadata": {
        "id": "HG_O3si6oDsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing -digits\n",
        "#DIGITS\n",
        "df['After digits removal'] = df['Reviews after Noise Removal'].apply(lambda y: ''.join([i for i in y if not i.isdigit()]))"
      ],
      "metadata": {
        "id": "mNOkgXbQo7Dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing -stop words\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "#STOP WORDS\n",
        "s = stopwords.words('english')\n",
        "df['Stopwords Removal'] = df['After digits removal'].apply(lambda x: \" \".join(x for x in x.split() if x not in s))"
      ],
      "metadata": {
        "id": "J0Akqs8fpAIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert to lower case\n",
        "#LOWER CASE\n",
        "df['Lower Case'] = df['Stopwords Removal'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
        "df"
      ],
      "metadata": {
        "id": "fy2J7-K1qRr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "\n",
        "from nltk.stem import PorterStemmer\n",
        "s = PorterStemmer()  \n",
        "#STEMMING\n",
        "df['After Stemming'] = df['Lower Case'].apply(lambda x: \" \".join([s.stem(word) for word in x]))"
      ],
      "metadata": {
        "id": "HQzKr0jhp33S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lemmatization\n",
        "\n",
        "from textblob import Word\n",
        "import nltk   #importing\n",
        "nltk.download('wordnet')\n",
        "#LEMMATIZATION\n",
        "\n",
        "df['After Lemmatization'] = df['After Stemming'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))"
      ],
      "metadata": {
        "id": "Scfb4-Vsp8PL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ym5WyAv2q1nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Avatar1_reviews.csv', index=False)"
      ],
      "metadata": {
        "id": "Y6HKHmRFrEI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Write a python program to conduct syntax and structure analysis of the clean text you just saved above. The syntax and structure analysis includes:(30 points). \n",
        "\n",
        "(1) Parts of Speech (POS) Tagging: Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) Constituency Parsing and Dependency Parsing: print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) Named Entity Recognition: Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ],
      "metadata": {
        "id": "gjLsh7_psKQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code here\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "p = []\n",
        "#for statement\n",
        "for sentence in df['Lower Case']:\n",
        "  text = word_tokenize(sentence)\n",
        "  p.append(nltk.pos_tag(text))\n",
        "p\n",
        "\n"
      ],
      "metadata": {
        "id": "VnhuJgR5r3ab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.2Constituency** **Parsing**"
      ],
      "metadata": {
        "id": "48fdode_c6PA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "YsQW6l4JykQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "#advanced version\n",
        "import tensorflow as tf\n",
        "#tensorflow\n",
        "import benepar   \n",
        "# benepar\n",
        "benepar.download('benepar_en3')"
      ],
      "metadata": {
        "id": "K6Si4k3ut5P4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy.cli\n",
        "#spacy.cli\n",
        "spacy.cli.download(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "e3oh98Ghu2fC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import benepar\n",
        "parser = benepar.Parser(\"benepar_en3\")\n",
        "for sentence in df['Lower Case']:\n",
        "  try:\n",
        "    tree = parser.parse(sentence)\n",
        "    print(tree)\n",
        "  except:\n",
        "    print(\"No Parse Tree\")\n",
        "    continue"
      ],
      "metadata": {
        "id": "ObJB4tGnvKzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Depedency Parsing\n",
        "\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "options = {'compact': True, 'font': 'Source Sans Pro', 'distance': 100}\n",
        "for sentence in df['Lower Case']:\n",
        "  doc = nlp(sentence)\n",
        "  displacy.render(doc, style = 'dep', options=options, jupyter=True)"
      ],
      "metadata": {
        "id": "xh6uvMiB2tiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.3 Named Entity Recognition**"
      ],
      "metadata": {
        "id": "VCO1n2OvdeLf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Named Entity Recognition\n",
        "\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "for sentence in df['Lower Case']:\n",
        "  doc = nlp(sentence)\n",
        "  for X in doc.ents:\n",
        "    if X.text and X.label_:\n",
        "      print([(X.text, X.label_)])"
      ],
      "metadata": {
        "id": "rvC2GdkatqpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here (Question 3-2), please explain the constituency parsing tree and dependency parsing tree."
      ],
      "metadata": {
        "id": "dtQBY49GrsUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Constituent parsing, also known as syntactical parsing, is the process of breaking down a text into smaller sub-texts and sub-phrases.\n",
        "This kind of parsing identifies the syntactical organization of the sentence under consideration. Sentence components, parts of speech tags, and sentence words make up this list.\n",
        "As an illustration, consider (S (NP (PRP I (ADVP (RB never))(VP (VBD left) (NP\n",
        "\n",
        "(SBAR (S (NP (PRP I (VP (VP (VBD registered) (NNP imdb) (NN account)) (NP (NN review) (NN title)) (NP (JJ good) (JJ bad))\n",
        "(Vent NNP) (NN opinion) (SBAR (S (NP (PRP I (VP (VBP want) (NP (NNS hours) (NN life)) (X (RB back)))))))))))))))))))))\n",
        "By treating each word as a node, dependency parsing describes the grammatical structure of a chosen phrase.\n",
        "Dependency grammar is used in this kind of parsing. Each word in this sentence is connected using their relationships.\n",
        "Intelligence, skills, metally, and illness are all present in the dependency parsing tree shown above. They both depend on talent, which comes before intelligence.'''"
      ],
      "metadata": {
        "id": "Q3ks2xcLsEOF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}